<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>5th Workshop on Image/Video/Audio Quality Assessment in Computer Vision, VLM and Diffusion Model</title>
    <link rel="stylesheet" href="styles.css">
    <script src="script.js"></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>5th Workshop on Image/Video/Audio Quality Assessment in Computer Vision, VLM and Diffusion Model</h1>
            <p></p>
            <!--<p>Workshop Date: Feb 28, 2025</p>-->
            <p>Workshop Date: Mar 6 or Mar 7, 2026 (TBD) </p>
            <p>Location: TBD</p>
            <p>Held in conjunction with <a href="https://wacv.thecvf.com/">WACV2026</a></p>
        </div>
    </header>

    <nav class="main-nav">
        <div class="menu-toggle">
            <div class="icon"></div>
        </div>
        <ul class="nav-list">
            <li><a href="index.html">Home</a></li>
	    <!--<li><a href="#Schedule">Schedule</a></li>-->
	    <!--<li><a href="#Keynotes">Keynotes</a></li>-->
            <li><a href="#Submission">Paper Submission</a></li>
            <!--<li><a href="#Competition">Competition</a></li>-->
            <li><a href="#Organizer">Organizer</a></li>
            <li><a href="#Information">Other information</a></li>
            <li><a href="#contact">Contact</a></li>
        </ul>
    </nav>


    <section id="Home" class="home-section">
        <h2>Home</h2>
        <p>5th Workshop on Image/Video/Audio Quality Assessment in Computer Vision, VLM and Diffusion Model</p>
        
        <h3>Important Dates/Links: </h3>
        <ul>
            <!--<li> Submission Deadline: <b style='color:red;'> 15 December, 2024 </b>-->
	    <li> Submission Deadline: <b style='color:red;'> 12/7/2025, 23:59 GMT </b>
	    <!--<li> Submission Link: <a href="https://cmt3.research.microsoft.com/ImageQuality2025">https://cmt3.research.microsoft.com/ImageQuality2025</a>-->
	    <li> Submission Link: <a href="https://openreview.net/group?id=thecvf.com%2FWACV%2F2026%2FWorkshop%2FWVAQ#tab-your-consoles
">OpenReviewLink</a>
            <!--<li> Acceptance Notification Deadline: <b style='color:red;'> 5 January, 2025</b>-->
            <li> Acceptance Notification Deadline: <b style='color:red;'> 1/1/2026 </b>
            <!--<li> Camera Ready Papers Submission Deadline: <b style='color:red;'> 10 January, 2025, at 11:59 PM Pacific Daylight Time.</b>-->
            <li> Camera Ready Papers Submission Deadline: <b style='color:red;'> 1/10/26 07:59 AM UTC </b>
	    <!--<li> Workshop Day: <b> 28 February, 2025 </b> -->
	    <li> Workshop Day: <b> 3/6/2026 or 3/7/2026 (TBD) </b> 
            <li> Workshop Website: <a href="https://wacv2026-image-quality-workshop.github.io/">https://wacv2026-image-quality-workshop.github.io/</a>
	    <li> Authors of accepted papers are required to present their work live, either in-person or remote. <b> If pre-recorded videos are used, the authors are required to join the meeting online to do Q&A live. </b>
	    <!--<li> The Microsoft CMT service was used for managing the peer-reviewing process for this conference. This service was provided for free by Microsoft and they bore all expenses, including costs for Azure cloud services as well as for software development and support.-->
	    <li> Accepted papers will be published in the WACV 2026 proceedings.
	    <li> Depending on the number of submissions we receive, author(s) of submitted papers may be required to review some papers. 
        </ul>

<h3> Description: </h3>
<p> 
Image, video, and audio quality significantly impacts machine learning and computer vision systems, yet remains underexplored by the broader research community. Real-world applications—from streaming services and autonomous vehicles to cashier-less stores and generative AI—critically depend on robust quality assessment and improvement techniques. Despite their importance, most visual learning systems assume high-quality inputs, while in reality, artifacts from capture, compression, transmission, and rendering processes can severely degrade performance and user experience.
</p>

<p> 
This workshop is particularly timely given the explosive growth of generative AI, which introduces new challenges in quality assessment for both inputs and outputs. By bringing together researchers from industry and academia, we aim to systematically investigate how quality issues affect various visual learning tasks and develop innovative assessment and mitigation techniques. Building on the success of our previous workshops at WACV(2022-2025), we expect to stimulate new research directions and attract more talent to this critical field, ultimately improving the robustness and reliability of computer vision applications across industries.
</p>

        <h3> Topics: </h3>
	<p>This workshop addresses topics related to image/video/audio quality assessment in machine learning, computer vision, VLM, Diffusion Model, and other types of generative AIs. The topics include, but are not limited to:</p>
        <ul>
			<li>Impact of image/video/audio quality in traditional machine learning and computer vision use cases such as object detection, segmentation, tracking, and recognition;
                        <li>Analyze, model and learn the quality impact from image/video/audio acquisition, compression, transcoding, transmission, decoding, rendering, and/or display;
                        <li>Techniques used to improve image/video/audio quality in terms of

                        <ul>
                            <li>brightening, color adjustment, sharpening, inpainting, deblurring, denoising, de-hazing, de-raining, demosaicing,
                            <li>removing artifacts such as shadows, glare, and reflections, etc.,
                            <li>resolution, frame rate, color gamut, dynamic range (SDR vs. HDR), etc.,
                            <li>noise/echo cancellation, speech enhancement, etc.;
                        </ul>
                    </li>
                    <li>Novel image/video/audio quality assessment methodologies: full reference, reduced-reference, and non-reference;
                    <li>Impact of image/video/audio quality in multi-modal use cases;
                    <li>Evaluate image/video/audio quality produced by generative AI;
		    <li>Evaluate the hallucination effects in image and video super-resolution and restoration using diffusion models;
                    <li>Techniques to measure the quality consistency across different types of content in video (such as ads, movies, streamed content, etc.);
                    <li>Datasets, statistics, and theory of image/video/audio quality;
                    <li>Research, applications and system development of the above.

        </ul>
    </section>

<!--<section id="Schedule" class="schedule-section">-->

    <!--<h2 style="text-align:center;">Schedule (MST)</h2>-->
    <!--<h3 style="text-align:center;">Feb 28th, 2025, 8:20 AM – 5:30 PM</h3>-->
    
    <!--<table>-->
	<!--<tr>-->
	    <!--<th class="time-column">Time</th>-->
	    <!--<th>Event</th>-->
	    <!--<th>Duration</th>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td>8:20-8:30am</td>-->
	    <!--<td>Opening Remarks (Host: Joe Liu)</td>-->
	    <!--<td>10 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td>8:30-9:30am</td>-->
            <!--<td>Keynote <br>-->
                <!--Keynote Speaker: Kevin Bowyer, "What makes a good quality face recognition training set?" (Host: Joe Liu)-->
            <!--</td>-->
	    <!--<td>60 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td>9:30-10:15am</td>-->
	    <!--<td>Coffee Break</td>-->
	    <!--<td>45 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td>10:15-11:15am</td>-->
	    <!--<td>Oral Long Session I (Host: Yarong Feng)</td>-->
	    <!--<td>60 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">10:15-10:30am</td>-->
	    <!--<td style="padding-left: 30px;">DaBiT: Depth and Blur informed Transformer for Video Deblurring (in person) Presenter: Crispian Morris</td>-->
	    <!--<td style="padding-left: 30px;">15 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">10:30-10:45am</td>-->
	    <!--<td style="padding-left: 30px;">Lights, Camera, Matching: The Role of Image Illumination in Fair Face Recognition (in person) Presenter: Gabriella Pangelinan</td>-->
	    <!--<td style="padding-left: 30px;">15 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">10:45-11:00am</td>-->
	    <!--<td style="padding-left: 30px;">Quantifying Generative Stability: Mode Collapse Entropy Score for Mode Diversity Evaluation (in person) Presenter: Jens Duym</td>-->
	    <!--<td style="padding-left: 30px;">15 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">11:00-11:15am</td>-->
	    <!--<td style="padding-left: 30px;">TE-NeRF: Triplane-Enhanced Neural Radiance Field for Artifact-Free Human Rendering (in person) Presenter: Sadia Mubashshira</td>-->
	    <!--<td style="padding-left: 30px;">15 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td>11:15-11:45am</td>-->
	    <!--<td>Oral Short Session I (Host: Yarong Feng)</td>-->
	    <!--<td>30 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">11:15-11:20pm</td>-->
	    <!--<td style="padding-left: 30px;">LatentPS: Image Editing Using Latent Representations in Diffusion Models (online) Presenter: Zilong Wu</td>-->
	    <!--<td style="padding-left: 30px;">5 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">11:20-11:25pm</td>-->
	    <!--<td style="padding-left: 30px;">IP-FaceDiff: Identity-Preserving Facial Video Editing with Diffusion (online) Presenter: Tharun Anand</td>-->
	    <!--<td style="padding-left: 30px;">5 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">11:25-11:30pm</td>-->
	    <!--<td style="padding-left: 30px;">Advancing Super-Resolution in Neural Radiance Fields via Variational Diffusion Strategies (online) Presenter: Shrey Vishen</td>-->
	    <!--<td style="padding-left: 30px;">5 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">11:30-11:35pm</td>-->
	    <!--<td style="padding-left: 30px;">Similarity Trajectories: Linking Sampling Process to Artifacts in Diffusion-Generated Images (online) Presenter: Dennis Menn</td>-->
	    <!--<td style="padding-left: 30px;">5 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">11:35-11:40pm</td>-->
	    <!--<td style="padding-left: 30px;">A Distortion Aware Image Quality Assessment Model (online) Presenter: Ha Thu Nguyen</td>-->
	    <!--<td style="padding-left: 30px;">5 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">11:40-11:45pm</td>-->
	    <!--<td style="padding-left: 30px;">SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing (online) Presenter: Youshan Zhang</td>-->
	    <!--<td style="padding-left: 30px;">5 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td>11:45am-1:15pm</td>-->
	    <!--<td>Lunch Break</td>-->
	    <!--<td>90 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td>1:15-2:00pm</td>-->
	    <!--<td>Oral Long Session II (Host: Qipin Chen)</td>-->
	    <!--<td>45 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">1:15-1:30pm</td>-->
	    <!--<td style="padding-left: 30px;">Unsupervised Generative Approach for Anomaly Detection to Enhance the Quality of Unseen Medical Datasets (in person) Presenter: Zhemin Zhang</td>-->
	    <!--<td style="padding-left: 30px;">15 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">1:30-1:45pm</td>-->
	    <!--<td style="padding-left: 30px;">Sparse Mixture-of-Experts for Non-Uniform Noise Reduction in MRI Images (in person) Presenter: Zeyun Deng</td>-->
	    <!--<td style="padding-left: 30px;">15 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">1:45-2:00pm</td>-->
	    <!--<td style="padding-left: 30px;">HipyrNet: Hypernet-Guided Feature Pyramid network for mixed-exposure correction (in person) Presenter: Aravind Shenoy</td>-->
	    <!--<td style="padding-left: 30px;">15 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td>2:00-2:15pm</td>-->
	    <!--<td>Challenge Introduction (Host: Xiaonan Zhao)</td>-->
	    <!--<td>15 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td>2:15-3:00pm</td>-->
	    <!--<td>Oral Long Session III (Host: Qipin Chen)</td>-->
	    <!--<td>45 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">2:15-2:30pm</td>-->
	    <!--<td style="padding-left: 30px;">Temporally Streaming Audio-Visual Synchronization for Real-World Videos (in person) Presenter: Jordan Voas</td>-->
	    <!--<td style="padding-left: 30px;">15 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">2:30-2:45pm</td>-->
	    <!--<td style="padding-left: 30px;">MambaTron: Efficient Cross-Modal Point Cloud Enhancement using Aggregate Selective State Space Modeling (in person) Presenter: Sai Tarun Inaganti</td>-->
	    <!--<td style="padding-left: 30px;">15 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">2:45-3:00pm</td>-->
	    <!--<td style="padding-left: 30px;">Diffusion Prism: Enhancing Diversity and Morphology Consistency in Mask-to-Image Diffusion (in person) Presenter: Hao Wang</td>-->
	    <!--<td style="padding-left: 30px;">15 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td>3:00-3:45pm</td>-->
	    <!--<td>Coffee Break</td>-->
	    <!--<td>45 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td>3:45-4:20pm</td>-->
	    <!--<td>Oral Short Session II (Host: Joe Liu)</td>-->
	    <!--<td>35 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">3:45-3:50pm</td>-->
	    <!--<td style="padding-left: 30px;">Mahalanobis k-NN: A Statistical Lens for Robust Point-Cloud Registrations (in person) Presenter: Tejas Anvekar</td>-->
	    <!--<td style="padding-left: 30px;">5 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">3:50-3:55pm</td>-->
	    <!--<td style="padding-left: 30px;">Improving Human Pose-Conditioned Generation: Fine-tuning ControlNet Models with Reinforcement Learning (online) Presenter: Jeonghwan Lee</td>-->
	    <!--<td style="padding-left: 30px;">5 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">3:55-4:00pm</td>-->
	    <!--<td style="padding-left: 30px;">PQD: POST-TRAINING QUANTIZATION FOR EFFICIENT DIFFUSION MODELS (online) Presenter: Jiaojiao Ye</td>-->
	    <!--<td style="padding-left: 30px;">5 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">4:00-4:05pm</td>-->
	    <!--<td style="padding-left: 30px;">High-Fidelity 4x Neural Reconstruction of Real-time Path Traced Images (in person) Presenter: Zhiqiang Lao</td>-->
	    <!--<td style="padding-left: 30px;">5 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">4:05-4:10pm</td>-->
	    <!--<td style="padding-left: 30px;">Confident Pseudo-labeled Diffusion Augmentation for Canine Cardiomegaly Detection (online) Presenter: Youshan Zhang</td>-->
	    <!--<td style="padding-left: 30px;">5 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">4:10-4:15pm</td>-->
	    <!--<td style="padding-left: 30px;">Revealing Palimpsests with Latent Diffusion Models: A Generative Approach to Image Inpainting and Handwriting Reconstruction (in person) Presenter: Mahdi Champour</td>-->
	    <!--<td style="padding-left: 30px;">5 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td style="padding-left: 30px;">4:15-4:20pm</td>-->
	    <!--<td style="padding-left: 30px;">LS-GAN: Human Motion Synthesis with Latent-space GANs (in person) Presenter: Avinash Amballa</td>-->
	    <!--<td style="padding-left: 30px;">5 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td>4:20-4:30pm</td>-->
	    <!--<td>Closing Remarks (Host: Joe Liu)</td>-->
	    <!--<td>10 mins</td>-->
	<!--</tr>-->
	<!--<tr>-->
	    <!--<td>4:30-5:30pm</td>-->
	    <!--<td>Poster Session</td>-->
	    <!--<td>60 mins</td>-->
	<!--</tr>-->
    <!--</table>-->

    <!--<table>-->
        <!--<tr>-->
            <!--<td>-->
        <!--Zoom Information for virtual presentations:-->
            <!--</td>-->
        <!--</tr>-->
        <!--<tr>-->
            <!--<td>-->
                
            <!--<p>Topic: VAQ Workshop - WACV2025</p>-->
            <!--<p>Time: Feb 28, 2025 08:15 AM </p>-->
            
            <!--<p>Join Zoom Meeting</p>-->
	    <!--<p>https://us06web.zoom.us/j/82700199161?pwd=SOAhhH2uWedGxcxvjn2vqRQD8tJmDW.1</p>-->
            
        <!--</td>-->
            
        <!--</tr>-->
    <!--</table>-->
<!--</section>-->

<!--<section id="Keynotes">-->
    <!--<h2>Keynotes</h2>-->

    <!--<div>-->
	<!--<h3>Keynote Speaker: Kevin Bowyer </h3>-->
        
        <!--[> Speaker Photo <]-->
	<!--<img class="circular--square" src="keynote_photo.png" alt="Keynote Speaker">-->

        <!--[> Talk Title <]-->
	<!--<h3>Title: "What makes a good quality face recognition training set?"</h3>-->

        <!--[> Abstract <]-->
	<!--<p>-->
	    <!--<strong>Abstract:</strong> This talk will start with comments on web-scraped, in-the-wild training sets.  And possibly also on CVPR and FG reviewing.  Then we will touch on the “identity problem” in training sets of images of persons who don’t exist.  Lastly, we will describe an example of a training set with targeted synthetic enhancements as a means of training set augmentation to solve a specific problem.  This talk should be entertaining and informative for a broad audience.-->
	<!--</p>-->

        <!--[> Bio <]-->
	<!--<p>-->
	    <!--<strong>Bio:</strong> Kevin Bowyer is the Schubmehl-Prein Family Professor of Computer Science and Engineering at the University of Notre Dame.  He is a Fellow of the AAAS, IEEE and IAPR, past EIC of the IEEE Transactions on Pattern Analysis and Machine Intelligence and the IEEE Transactions on Biometrics, Behavior, and Identity Science, recipient of a Technical Achievement Award from the IEEE Computer Society, and of the Meritorious Service Award and the Leadership Award from the IEEE Biometrics Council.-->
	<!--</p>-->
    <!--</div>-->

<!--</section>-->

    <section id="Submission">
        <h2>Submission Guidelines and Review Process:</h2>
        <ul>
            <li>Authors are encouraged to submit high-quality, original (i.e., not been previously published or accepted for publication in substantially similar form in any peer-reviewed venue including journal, conference or workshop) research.
            <li>All submissions should follow the same template as for the main WACV2026 conference. Please refer to the <a href="https://media.eventhosts.cc/Conferences/WACV2026/wacv-2026-author-kit-template.zip">WACV Author Kit</a>. 
            <li>The main paper has an 8-page limit, references do not count toward this. There is no limit on the number of pages in the supplementary material. Only pdf files are accepted.
            <li>Unlike the main conference(WACV2026), the review process for this workshop has only one round, and is single-blind. Authors do not have to be anonymized when submitting their work.
	    <li>We will be using OpenReview to manage submissions. Submissions under review will be visible only to their assigned members of the program committee (area chairs and reviewers). The reviews and author responses will never be made public, and we will not be soliciting comments from the general public during the reviewing process.
Anyone who plans to submit a paper as an author or a co-author will need to create (or update) their OpenReview profile by the full paper submission deadline. Please note that if you use a non-institutional email address such as gmail.com, it can take up to two weeks to approve your OpenReview profile. Profiles tied to institutional email addresses are generally approved automatically. Plan accordingly. By submitting a paper to the workshop, the authors agree to the review process and understand that papers are processed by the OpenReview system to match each manuscript to the best possible area chairs and reviewers.
	    <li>Please submit your paper via this link: <a href="https://openreview.net/group?id=thecvf.com%2FWACV%2F2026%2FWorkshop%2FWVAQ#tab-your-consoles">OpenReviewLink</a>
            <!--<li>Authors of accepted papers will be notified via email by: <b> 5 January, 2025 </b>-->
            <li>Authors of accepted papers will be notified via email by: <b> 1/1/2026 </b>
        </ul>
    </section>

    <section id="Organizer">
        <h2>Organizers</h2>
        <div class="organizer-row">
            <div class="organizer">
		<img src="organizer_yarong.png" alt="Organizer 1">
                <h3>Yarong Feng</h3>
                <p>Amazon</p>
            </div>
            <div class="organizer">
		<img src="organizer_joe.png" alt="Organizer 3">
                <h3>Joe Liu</h3>
                <p>Amazon</p>
            </div>
            <div class="organizer">
		<img src="organizer_qipin.png" alt="Organizer 2">
                <h3>Qipin Chen</h3>
                <p>Amazon</p>
            </div>
        </div>
    </section>

    <section id="Information">
        <h2>Information</h2>
        <p>TBD</p>
        </table>
    </section>

    <section id="contact">
        <h2>Contact Us</h2>
        <!--<p>If you have any questions or inquiries, please contact us at <a href="mailto:wacv2025-image-quality-workshop2@amazon.com">wacv2025-image-quality-workshop2@amazon.com</a>.</p>-->
        <p>If you have any questions or inquiries, please contact us at <a href="mailto:wacv2026-image-quality-workshop@amazon.com">wacv2026-image-quality-workshop@amazon.com</a>.</p>
    </section>

    <footer>
        <p>&copy; 5th Workshop on Image/Video/Audio Quality Assessment in Computer Vision, VLM and Diffusion Model. All rights reserved.</p>
    </footer>
</body>
</html>
