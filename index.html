<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>5th Workshop on Image/Video/Audio Quality Assessment in Computer Vision, VLM and Diffusion Model</title>
    <link rel="stylesheet" href="styles.css">
    <script src="script.js"></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>5th Workshop on Image/Video/Audio Quality Assessment in Computer Vision, VLM and Diffusion Model</h1>
            <p></p>
            <p>Workshop Date: Mar 7, 2026 </p>
            <p>Location: TBD</p>
            <p>Held in conjunction with <a href="https://wacv.thecvf.com/">WACV2026</a></p>
        </div>
    </header>

    <nav class="main-nav">
        <div class="menu-toggle">
            <div class="icon"></div>
        </div>
        <ul class="nav-list">
            <li><a href="index.html">Home</a></li>
	    <li><a href="#Schedule">Schedule</a></li>
	    <li><a href="#Keynotes">Keynotes</a></li>
            <li><a href="#Submission">Paper Submission</a></li>
            <!--<li><a href="#Competition">Competition</a></li>-->
            <li><a href="#Organizer">Organizer</a></li>
            <li><a href="#Information">Other information</a></li>
            <li><a href="#contact">Contact</a></li>
        </ul>
    </nav>


    <section id="Home" class="home-section">
        <h2>Home</h2>
        <p>5th Workshop on Image/Video/Audio Quality Assessment in Computer Vision, VLM and Diffusion Model</p>
        
        <h3>Important Dates/Links: </h3>
        <ul>
            <!--<li> Submission Deadline: <b style='color:red;'> 15 December, 2024 </b>-->
	    <li> Submission Deadline: <b style='color:red;'> 12/15/2025, 23:59 GMT </b>
	    <!--<li> Submission Link: <a href="https://cmt3.research.microsoft.com/ImageQuality2025">https://cmt3.research.microsoft.com/ImageQuality2025</a>-->
	    <li> Submission Link: <a href="https://openreview.net/group?id=thecvf.com%2FWACV%2F2026%2FWorkshop%2FWVAQ#tab-your-consoles
">OpenReviewLink</a>
            <!--<li> Acceptance Notification Deadline: <b style='color:red;'> 5 January, 2025</b>-->
            <li> Acceptance Notification Deadline: <b style='color:red;'> 1/1/2026 </b>
            <!--<li> Camera Ready Papers Submission Deadline: <b style='color:red;'> 10 January, 2025, at 11:59 PM Pacific Daylight Time.</b>-->
            <li> Camera Ready Papers Submission Deadline: <b style='color:red;'> 1/10/26 07:59 AM UTC </b>
	    <!--<li> Workshop Day: <b> 28 February, 2025 </b> -->
	    <li> Workshop Day: <b> 3/7/2026 </b> 
            <li> Workshop Website: <a href="https://wacv2026-image-quality-workshop.github.io/">https://wacv2026-image-quality-workshop.github.io/</a>
	    <li> Authors of accepted papers are required to present their work live, either in-person or remote. <b> If pre-recorded videos are used, the authors are required to join the meeting online to do Q&A live. </b>
	    <!--<li> The Microsoft CMT service was used for managing the peer-reviewing process for this conference. This service was provided for free by Microsoft and they bore all expenses, including costs for Azure cloud services as well as for software development and support.-->
	    <li> Accepted papers will be published in the WACV 2026 proceedings.
	    <li> Depending on the number of submissions we receive, author(s) of submitted papers may be required to review some papers. 
        </ul>

<h3> Description: </h3>
<p> 
Image, video, and audio quality significantly impacts machine learning and computer vision systems, yet remains underexplored by the broader research community. Real-world applications—from streaming services and autonomous vehicles to cashier-less stores and generative AI—critically depend on robust quality assessment and improvement techniques. Despite their importance, most visual learning systems assume high-quality inputs, while in reality, artifacts from capture, compression, transmission, and rendering processes can severely degrade performance and user experience.
</p>

<p> 
This workshop is particularly timely given the explosive growth of generative AI, which introduces new challenges in quality assessment for both inputs and outputs. By bringing together researchers from industry and academia, we aim to systematically investigate how quality issues affect various visual learning tasks and develop innovative assessment and mitigation techniques. Building on the success of our previous workshops at WACV(2022-2025), we expect to stimulate new research directions and attract more talent to this critical field, ultimately improving the robustness and reliability of computer vision applications across industries.
</p>

        <h3> Topics: </h3>
	<p>This workshop addresses topics related to image/video/audio quality assessment in machine learning, computer vision, VLM, Diffusion Model, and other types of generative AIs. The topics include, but are not limited to:</p>
        <ul>
			<li>Impact of image/video/audio quality in traditional machine learning and computer vision use cases such as object detection, segmentation, tracking, and recognition;
                        <li>Analyze, model and learn the quality impact from image/video/audio acquisition, compression, transcoding, transmission, decoding, rendering, and/or display;
                        <li>Techniques used to improve image/video/audio quality in terms of

                        <ul>
                            <li>brightening, color adjustment, sharpening, inpainting, deblurring, denoising, de-hazing, de-raining, demosaicing,
                            <li>removing artifacts such as shadows, glare, and reflections, etc.,
                            <li>resolution, frame rate, color gamut, dynamic range (SDR vs. HDR), etc.,
                            <li>noise/echo cancellation, speech enhancement, etc.;
                        </ul>
                    </li>
                    <li>Novel image/video/audio quality assessment methodologies: full reference, reduced-reference, and non-reference;
                    <li>Impact of image/video/audio quality in multi-modal use cases;
                    <li>Evaluate image/video/audio quality produced by generative AI;
		    <li>Evaluate the hallucination effects in image and video super-resolution and restoration using diffusion models;
                    <li>Techniques to measure the quality consistency across different types of content in video (such as ads, movies, streamed content, etc.);
                    <li>Datasets, statistics, and theory of image/video/audio quality;
                    <li>Research, applications and system development of the above.

        </ul>
    </section>

<section id="Schedule" class="schedule-section">

    <h2 style="text-align:center;">Schedule (MST)</h2>
    <h3 style="text-align:center;">Mar 7th, 2026, 8:20 AM – 6:00 PM</h3>
    
    <table>
	<tr>
	    <th class="time-column">Time</th>
	    <th>Event</th>
	    <th>Duration</th>
	</tr>
	<tr>
	    <td>8:20-8:30am</td>
	    <td>Opening Remarks (Host: Joe Liu)</td>
	    <td>10 mins</td>
	</tr>
	<tr>
	    <td>8:30-9:30am</td>
            <td>Keynote <br>
                Keynote Speaker: Gérard G. Medioni (Host: Joe Liu)
            </td>
	    <td>60 mins</td>
	</tr>
	<tr>
	    <td>9:30-10:15am</td>
	    <td>Coffee Break</td>
	    <td>45 mins</td>
	</tr>
	<tr>
	    <td>10:15-11:45am</td>
	    <td>Oral Long Session I (Host: Yarong Feng)</td>
	    <td>90 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">10:15-10:30am</td>
	    <td style="padding-left: 30px;">Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior (in person)</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">10:30-10:45am</td>
	    <td style="padding-left: 30px;">REMinD: Balancing Robust Concept Unlearning and Image Quality in Diffusion Models (in person)</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">10:45-11:00am</td>
	    <td style="padding-left: 30px;">Reason Then Ground: Multilingual Text/Logo Grounding on Movie Posters (in person)</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">11:00-11:15am</td>
	    <td style="padding-left: 30px;">VideoForge: Efficient Domain Adaptation for Video Generation Through Quality-Driven Rewards and Enhanced LoRA (in person)</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">11:15-11:30am</td>
	    <td style="padding-left: 30px;">HandSurge: Localized Neural Surgery for Diffusion-Generated Hand Deformity Restoration (in person)</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">11:30-11:45am</td>
	    <td style="padding-left: 30px;">HiFi-Deblur: High-Frequency Intense Image Deblurring with Frequency-Decoupled U-Net and Discrete Wavelet Transform (in person)</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td>11:45am-1:00pm</td>
	    <td>Lunch Break</td>
	    <td>75 mins</td>
	</tr>
	<tr>
	    <td>1:00-2:00pm</td>
	    <td>Oral Long Session II (Host: Qipin Chen)</td>
	    <td>60 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">1:00-1:15pm</td>
	    <td style="padding-left: 30px;">Motion Blur Detection and Segmentation from Static Image Artworks (in person)</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">1:15-1:30pm</td>
	    <td style="padding-left: 30px;">Transforming Video Subjective Testing with Training, Engagement, and Real-Time Feedback (in person)</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">1:30-1:45pm</td>
	    <td style="padding-left: 30px;">Can You Find the Difference? Visually Identical Image Detection (in person)</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">1:45-2:00pm</td>
	    <td style="padding-left: 30px;">Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks (in person)</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td>2:00-3:00pm</td>
            <td>Keynote <br>
                Keynote Speaker: Sarah Ostadabbas, "Toward Data-Efficient Dynamically-Aware Visual Intelligence" (Host: Joe Liu)
            </td>
	    <td>60 mins</td>
	</tr>
	<tr>
	    <td>3:00-3:45pm</td>
	    <td>Coffee Break</td>
	    <td>45 mins</td>
	</tr>
	<tr>
	    <td>3:45-4:45pm</td>
	    <td>Oral Short Session I (Host: Qipin Chen)</td>
	    <td>60 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">3:45-3:52pm</td>
	    <td style="padding-left: 30px;">Diffuse4D: Completing NeRF-Stereo Depth via Diffusion-Driven Restoration in Dynamic Scenes (in person)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">3:52-3:59pm</td>
	    <td style="padding-left: 30px;">Seeing in the Dark: Synthesizing Underexposure for More Robust Underwater Image Augmentation (in person)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">3:59-4:06pm</td>
	    <td style="padding-left: 30px;">ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers (in person)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">4:06-4:13pm</td>
	    <td style="padding-left: 30px;">Cost Savings from Automatic Quality Assessment of Generated Images (in person)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">4:13-4:20pm</td>
	    <td style="padding-left: 30px;">VIBEFACE - Video and Image Biometric Dataset for Evaluation of Faces (in person)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">4:20-4:27pm</td>
	    <td style="padding-left: 30px;">We Still See Broken Limbs: Towards Anatomical Realism in GenAI via Human Preference Learning (in person)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">4:27-4:34pm</td>
	    <td style="padding-left: 30px;">JetBench: Quality-Aware Benchmarking of Vision Models for Jet Parameter Classification in Heavy-Ion Physics (in person)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">4:34-4:41pm</td>
	    <td style="padding-left: 30px;">STEC: A Spatio-Temporal Entropy Coverage Metric for Evaluating Sampled Video Frames (in person)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">4:41-4:48pm</td>
	    <td style="padding-left: 30px;">SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models (in person)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td>4:45-5:00pm</td>
	    <td>Closing Remarks (Host: Joe Liu)</td>
	    <td>15 mins</td>
	</tr>
	<tr>
	    <td>5:00-6:00pm</td>
	    <td>Poster Session + Online Oral Presentations</td>
	    <td>60 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">5:00-5:07pm</td>
	    <td style="padding-left: 30px;">From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance (online)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">5:07-5:14pm</td>
	    <td style="padding-left: 30px;">Device-Robust Spectral Grading and Origin Detection from UV-Vis-NIR Images: Towards Practical Gemstone Quality Assessment (online)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">5:14-5:21pm</td>
	    <td style="padding-left: 30px;">Vision Language Models Learn to Assess Images with Specialists (online)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">5:21-5:28pm</td>
	    <td style="padding-left: 30px;">When Probe and Gallery are Low Quality: Decreasing Accuracy and Increasing Demographic Disparities in 1:N Identification (online)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">5:28-5:35pm</td>
	    <td style="padding-left: 30px;">CARLA-Haze: A Synthetic Benchmark for Outdoor Image Dehazing (online)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">5:35-5:42pm</td>
	    <td style="padding-left: 30px;">Quality-Driven and Diversity-Aware Sample Expansion for Robust Marine Obstacle Segmentation (online)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">5:42-5:49pm</td>
	    <td style="padding-left: 30px;">Enhancement as Augmentation: Improving Detection in Highly Degraded Underwater Images Through Mixed-Domain Training (online)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">5:49-5:56pm</td>
	    <td style="padding-left: 30px;">Image-Specific Adaptation of Transformer Encoders for Compute-Efficient Segmentation (online)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">5:56-6:03pm</td>
	    <td style="padding-left: 30px;">YOLO-OSA: A ShuffleAttention-Enhanced YOLO Model for FOD Detection with Comprehensive Benchmarking on MS COCO (online)</td>
	    <td style="padding-left: 30px;">7 mins</td>
	</tr>
    </table>

    <table>
        <tr>
            <td>
        Zoom Information for virtual presentations: TBD
            </td>
        </tr>
    </table>
</section>

<section id="Keynotes">
    <h2>Keynotes</h2>

    <div>
	<h3>Keynote Speaker: Gérard G. Medioni, VP & Distinguished Scientist</h3>
        
	<!--[> Speaker Photo <]-->
	<img class="circular--square" src="medioni.png" alt="Keynote Speaker" width="300" height="400">

        <!--[> Talk Title <]-->
	<h3>Title: "Prime Video: Delivering High Quality Streaming"</h3>

        <!--[> Abstract <]-->
	<p>
	    <strong>Abstract:</strong> Gerard Medioni will present an exploration of the cutting-edge technology driving the Prime Video experience. The session opens with three flagship innovations: AI-aided dubbing, video season recaps, and a newly launched NBA feature — each showcasing how Prime Video is pushing the boundaries of content delivery and personalization.
	    Gerard will then take a deeper dive into three areas of active technical development: the challenges and solutions behind delivering streaming video in vertical format for mobile audiences; Prime Video's approach to detecting and classifying audio quality defects; and the unique image quality challenges inherent to livestreaming — and how the team is tackling them.
	</p>

	<!--[> Bio <]-->
	<p>
	    <strong>Bio:</strong> Gérard G. Medioni is a computer scientist, author, academic and inventor. He is a vice president and distinguished scientist at Amazon and serves as emeritus professor of Computer Science at the University of Southern California. Medioni has made contributions to computer vision, in particular 3D sensing, surface reconstruction, and object modelling. He has translated his computer vision research into customer-facing inventions and products. He has authored four books, including Emerging Topics in Computer Vision, Multimedia Systems: Algorithms, Standards, and Industry Practices, and A Computational Framework for Segmentation and Grouping, and has published more than 80 journal papers, 200 conference papers, with over 34,000 citations and his h-index is 88. In addition, he holds 123 patents to his name which include Visual tracking in video images in unconstrained environments by exploiting on-the-fly context using supporters and distracters and Depth mapping based on pattern matching and stereoscopic information, along with patents on Just Walk Out technology and Amazon One. Medioni is a Fellow of the Association for the Advancement of Artificial Intelligence, the Institute of Electrical and Electronics Engineers, the International Association for Pattern Recognition, and the National Academy of Inventors. He is also a member of National Academy of Engineering.
	</p>
    </div>

	<hr>


    <div>
	<h3>Keynote Speaker: Sarah Ostadabbas, Associate Professor</h3>
        
	<!--[> Speaker Photo <]-->
	<img class="circular--square" src="sarah.png" alt="Keynote Speaker" width="300" height="400">

        <!--[> Talk Title <]-->
	<h3>Title: "Toward Data-Efficient Dynamically-Aware Visual Intelligence"</h3>

	<!--[> Abstract <]-->
	<p>
	    <strong>Abstract:</strong> Despite rapid advances in multimodal foundation models, today's video AI systems still struggle to reason about motion, causality, and physical change, especially in real-world, small-data environments. This talk argues that scaling data and parameters alone yields models that reproduce appearance, but fail to anticipate how the world evolves. Instead, I advocate for a shift toward motion-grounded visual intelligence, where dynamics (not static frames or language priors) form the core representation. I will present recent work from our lab demonstrating how motion provides a low-dimensional bridge between pixels and physics, enabling systems that discover, describe, and generate video through dynamics-aware reasoning. Using examples from our motion-aware zero-prompt video understanding and our physics-grounded generative framework, I show how treating video as a learnable world model, rather than a sequence of images, supports more robust generalization, interpretable reasoning, and physically consistent generation. The talk concludes with a broader vision for Physical AI: data-efficient systems that learn from motion, reason over future states, and operate safely in unconstrained environments such as healthcare, robotics, and human-centered applications.
	</p>

	<!--[> Bio <]-->
	<p>
	    <strong>Bio:</strong> Professor Ostadabbas is an associate professor in the Electrical and Computer Engineering Department at Northeastern University (NU) in Boston, Massachusetts, USA. She joined NU in 2016 after completing her post-doctoral research at Georgia Tech, following the achievement of her PhD at the University of Texas at Dallas in 2014. At NU, Professor Ostadabbas holds the roles of Director at the Augmented Cognition Laboratory (ACLab), Director of Women in Engineering (WIE), and Co-Director at The Center for Signal Processing, Imaging, Reasoning, and Learning (SPIRAL). Her research focuses on the convergence of computer vision and machine learning, particularly emphasizing representation learning in visual perception problems. In her applied research, she has significantly contributed to the understanding, detection, and prediction of human and animal behaviors through the modeling of visual motion, considering various biomechanical factors. Professor Ostadabbas also extends her work to the Small Data Domain, including applications in medical and military fields, where data collection and labeling are costly and protected by strict privacy laws. Her solutions involve deep learning frameworks that operate effectively with limited labeled training data, incorporate domain knowledge for prior learning and synthetic data augmentation, and enhance the generalization of learning across domains by acquiring invariant representations. Professor Ostadabbas has co-authored over 130 peer-reviewed journal and conference articles and received research awards from prestigious institutions such as the National Science Foundation (NSF), Department of Defense (DoD), Sony, Mathworks, Amazon AWS, Verizon, Oracle, Biogen, and NVIDIA. She has been honored with the NSF CAREER Award (2022), Sony Faculty Innovation Award (2023), was the runner-up for the Oracle Excellence Award (2023), and One of the 120+ Women Spearheading Advances in Visual Tech and AI Recognized by LDV Capital (2024). She served in the organization committees of many workshops in renowned conferences (such as CVPR, ECCV, ICCV, ICIP, ICCASP, BioCAS, CHASE, ICHI) in various roles including Lead/Co-Lead Organizer, Program Chair, Board Member, Publicity Co-Chair, Session Chair, Technical Committee, and Mentor.
	</p>
    </div>

</section>

    <section id="Submission">
        <h2>Submission Guidelines and Review Process:</h2>
        <ul>
            <li>Authors are encouraged to submit high-quality, original (i.e., not been previously published or accepted for publication in substantially similar form in any peer-reviewed venue including journal, conference or workshop) research.
            <li>All submissions should follow the same template as for the main WACV2026 conference. Please refer to the <a href="https://media.eventhosts.cc/Conferences/WACV2026/wacv-2026-author-kit-template.zip">WACV Author Kit</a>. 
            <li>The main paper has an 8-page limit, references do not count toward this. There is no limit on the number of pages in the supplementary material. Only pdf files are accepted.
            <li>Unlike the main conference(WACV2026), the review process for this workshop has only one round, and is single-blind. Authors do not have to be anonymized when submitting their work.
	    <li>We will be using OpenReview to manage submissions. Submissions under review will be visible only to their assigned members of the program committee (area chairs and reviewers). The reviews and author responses will never be made public, and we will not be soliciting comments from the general public during the reviewing process.
Anyone who plans to submit a paper as an author or a co-author will need to create (or update) their OpenReview profile by the full paper submission deadline. Please note that if you use a non-institutional email address such as gmail.com, it can take up to two weeks to approve your OpenReview profile. Profiles tied to institutional email addresses are generally approved automatically. Plan accordingly. By submitting a paper to the workshop, the authors agree to the review process and understand that papers are processed by the OpenReview system to match each manuscript to the best possible area chairs and reviewers.
	    <li>Please submit your paper via this link: <a href="https://openreview.net/group?id=thecvf.com%2FWACV%2F2026%2FWorkshop%2FWVAQ#tab-your-consoles">OpenReviewLink</a>
            <!--<li>Authors of accepted papers will be notified via email by: <b> 5 January, 2025 </b>-->
            <li>Authors of accepted papers will be notified via email by: <b> 1/1/2026 </b>
        </ul>
    </section>

    <section id="Organizer">
        <h2>Organizers</h2>
        <div class="organizer-row">
            <div class="organizer">
		<img src="organizer_yarong.png" alt="Organizer 1">
                <h3>Yarong Feng</h3>
                <p>Amazon</p>
            </div>
            <div class="organizer">
		<img src="organizer_joe.png" alt="Organizer 3">
                <h3>Joe Liu</h3>
                <p>Amazon</p>
            </div>
            <div class="organizer">
		<img src="organizer_qipin.png" alt="Organizer 2">
                <h3>Qipin Chen</h3>
                <p>Amazon</p>
            </div>
        </div>
    </section>

    <section id="Information">
        <h2>Information</h2>
        <p>TBD</p>
        </table>
    </section>

    <section id="contact">
        <h2>Contact Us</h2>
        <!--<p>If you have any questions or inquiries, please contact us at <a href="mailto:wacv2025-image-quality-workshop2@amazon.com">wacv2025-image-quality-workshop2@amazon.com</a>.</p>-->
        <p>If you have any questions or inquiries, please contact us at <a href="mailto:wacv2026-image-quality-workshop@amazon.com">wacv2026-image-quality-workshop@amazon.com</a>.</p>
    </section>

    <footer>
        <p>&copy; 5th Workshop on Image/Video/Audio Quality Assessment in Computer Vision, VLM and Diffusion Model. All rights reserved.</p>
    </footer>
</body>
</html>
