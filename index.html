<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>5th Workshop on Image/Video/Audio Quality Assessment in Computer Vision, VLM and Diffusion Model</title>
    <link rel="stylesheet" href="styles.css">
    <script src="script.js"></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>5th Workshop on Image/Video/Audio Quality Assessment in Computer Vision, VLM and Diffusion Model</h1>
            <p></p>
            <!--<p>Workshop Date: Feb 28, 2025</p>-->
            <p>Workshop Date: Mar 6 or Mar 7, 2026 (TBD)</p>
            <p>Location: TBD</p>
            <p>Held in conjunction with <a href="https://wacv.thecvf.com/">WACV2026</a></p>
        </div>
    </header>

    <nav class="main-nav">
        <div class="menu-toggle">
            <div class="icon"></div>
        </div>
        <ul class="nav-list">
            <li><a href="index.html">Home</a></li>
	    <li><a href="#Schedule">Schedule</a></li>
	    <li><a href="#Keynotes">Keynotes</a></li>
            <li><a href="#Submission">Paper Submission</a></li>
            <!--<li><a href="#Competition">Competition</a></li>-->
            <li><a href="#Organizer">Organizer</a></li>
            <li><a href="#Information">Other information</a></li>
            <li><a href="#contact">Contact</a></li>
        </ul>
    </nav>


    <section id="Home" class="home-section">
        <h2>Home</h2>
        <p>5th Workshop on Image/Video/Audio Quality Assessment in Computer Vision, VLM and Diffusion Model</p>
        
        <h3>Important Dates/Links: </h3>
        <ul>
            <!--<li> Submission Deadline: <b style='color:red;'> 15 December, 2024 </b>-->
	    <li> Submission Deadline: <b style='color:red;'> <SPAN STYLE="text-decoration:line-through">15 December, 2024</span> -> 21 December, 2024</b>
	    <li> Submission Link: <a href="https://cmt3.research.microsoft.com/ImageQuality2025">https://cmt3.research.microsoft.com/ImageQuality2025</a>
            <li> Acceptance Notification Deadline: <b style='color:red;'> 5 January, 2025</b>
            <li> Camera Ready Papers Submission Deadline: <b style='color:red;'> 10 January, 2025, at 11:59 PM Pacific Daylight Time.</b>
	    <li> Workshop Day: <b> 28 February, 2025 </b> 
            <li> Workshop Website: <a href="https://wacv2025-image-quality-workshop2.github.io/index.html">https://wacv2025-image-quality-workshop2.github.io/index.html</a>
	    <li> Authors of accepted papers are required to present their work live, either in-person or remote. <b> If pre-recorded videos are used, the authors are required to join the meeting online to do Q&A live. </b>
        </ul>

<h3> Description: </h3>
<p> Many machine learning tasks and computer vision algorithms are susceptible to image/video/audio quality artifacts. Nonetheless, most visual learning and vision systems assume high-quality image/video/audio as input. In reality, noises and distortions are common in image/video/audio capturing and acquisition process. Oftentimes, artifacts can be introduced in the video compression, transcoding, transmission, decoding, and/or rendering process. All of these quality issues play a critical role on the performance of learning algorithms, systems and applications, therefore could directly impact the customer experience.
</p>
        <h3> Topics: </h3>
        <p>This workshop addresses topics related to image/video/audio quality in machine learning, computer vision, and generative AI. The topics include, but are not limited to:</p>
        <ul>
                        <li>Impact of image/video/audio quality in traditional machine learning and computer vision use cases such as object detection, segmentation, tracking, and recognition;
                        <li>Analyze, model and learn the quality impact from image/video/audio acquisition, compression, transcoding, transmission, decoding, rendering, and/or display;
                        <li>Techniques used to improve image/video/audio quality in terms of

                        <ul>
                            <li>brightening, color adjustment, sharpening, inpainting, deblurring, denoising, de-hazing, de-raining, demosaicing,
                            <li>removing artifacts such as shadows, glare, and reflections, etc.,
                            <li>resolution, frame rate, color gamut, dynamic range (SDR vs. HDR), etc.,
                            <li>noise/echo cancellation, speech enhancement, etc.;
			    <li>Film grain preservation and synthesis.
                        </ul>
                    </li>
                    <li>Novel image/video/audio quality assessment methodologies: full reference, reduced-reference, and non-reference;
                    <li>Impact of image/video/audio quality in multi-modal use cases;
                    <li>Evaluate image/video/audio quality produced by generative AI;
                    <li>Techniques to detect and mitigate audio/video synchronization issue;
                    <li>Techniques to measure the quality consistency across different types of content in video (such as ads, movies, streamed content, etc.);
		    <li>Subjective image/video/audio quality data creation, cleaning, and validation in both lab and crowd-sourcing setups;
                    <li>Datasets, statistics, and theory of image/video/audio quality;
                    <li>Research, applications and system development of the above.

        </ul>
    </section>

	<section id="Schedule" class="schedule-section">

    <h2 style="text-align:center;">Schedule (MST)</h2>
    <h3 style="text-align:center;">Feb 28th, 2025, 8:20 AM – 5:30 PM</h3>
    
    <table>
	<tr>
	    <th class="time-column">Time</th>
	    <th>Event</th>
	    <th>Duration</th>
	</tr>
	<tr>
	    <td>8:20-8:30am</td>
	    <td>Opening Remarks (Host: Joe Liu)</td>
	    <td>10 mins</td>
	</tr>
	<tr>
	    <td>8:30-9:30am</td>
            <td>Keynote <br>
                Keynote Speaker: Kevin Bowyer, "What makes a good quality face recognition training set?" (Host: Joe Liu)
            </td>
	    <td>60 mins</td>
	</tr>
	<tr>
	    <td>9:30-10:15am</td>
	    <td>Coffee Break</td>
	    <td>45 mins</td>
	</tr>
	<tr>
	    <td>10:15-11:15am</td>
	    <td>Oral Long Session I (Host: Yarong Feng)</td>
	    <td>60 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">10:15-10:30am</td>
	    <td style="padding-left: 30px;">DaBiT: Depth and Blur informed Transformer for Video Deblurring (in person) Presenter: Crispian Morris</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">10:30-10:45am</td>
	    <td style="padding-left: 30px;">Lights, Camera, Matching: The Role of Image Illumination in Fair Face Recognition (in person) Presenter: Gabriella Pangelinan</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">10:45-11:00am</td>
	    <td style="padding-left: 30px;">Quantifying Generative Stability: Mode Collapse Entropy Score for Mode Diversity Evaluation (in person) Presenter: Jens Duym</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">11:00-11:15am</td>
	    <td style="padding-left: 30px;">TE-NeRF: Triplane-Enhanced Neural Radiance Field for Artifact-Free Human Rendering (in person) Presenter: Sadia Mubashshira</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td>11:15-11:45am</td>
	    <td>Oral Short Session I (Host: Yarong Feng)</td>
	    <td>30 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">11:15-11:20pm</td>
	    <td style="padding-left: 30px;">LatentPS: Image Editing Using Latent Representations in Diffusion Models (online) Presenter: Zilong Wu</td>
	    <td style="padding-left: 30px;">5 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">11:20-11:25pm</td>
	    <td style="padding-left: 30px;">IP-FaceDiff: Identity-Preserving Facial Video Editing with Diffusion (online) Presenter: Tharun Anand</td>
	    <td style="padding-left: 30px;">5 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">11:25-11:30pm</td>
	    <td style="padding-left: 30px;">Advancing Super-Resolution in Neural Radiance Fields via Variational Diffusion Strategies (online) Presenter: Shrey Vishen</td>
	    <td style="padding-left: 30px;">5 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">11:30-11:35pm</td>
	    <td style="padding-left: 30px;">Similarity Trajectories: Linking Sampling Process to Artifacts in Diffusion-Generated Images (online) Presenter: Dennis Menn</td>
	    <td style="padding-left: 30px;">5 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">11:35-11:40pm</td>
	    <td style="padding-left: 30px;">A Distortion Aware Image Quality Assessment Model (online) Presenter: Ha Thu Nguyen</td>
	    <td style="padding-left: 30px;">5 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">11:40-11:45pm</td>
	    <td style="padding-left: 30px;">SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing (online) Presenter: Youshan Zhang</td>
	    <td style="padding-left: 30px;">5 mins</td>
	</tr>
	<tr>
	    <td>11:45am-1:15pm</td>
	    <td>Lunch Break</td>
	    <td>90 mins</td>
	</tr>
	<tr>
	    <td>1:15-2:00pm</td>
	    <td>Oral Long Session II (Host: Qipin Chen)</td>
	    <td>45 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">1:15-1:30pm</td>
	    <td style="padding-left: 30px;">Unsupervised Generative Approach for Anomaly Detection to Enhance the Quality of Unseen Medical Datasets (in person) Presenter: Zhemin Zhang</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">1:30-1:45pm</td>
	    <td style="padding-left: 30px;">Sparse Mixture-of-Experts for Non-Uniform Noise Reduction in MRI Images (in person) Presenter: Zeyun Deng</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">1:45-2:00pm</td>
	    <td style="padding-left: 30px;">HipyrNet: Hypernet-Guided Feature Pyramid network for mixed-exposure correction (in person) Presenter: Aravind Shenoy</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td>2:00-2:15pm</td>
	    <td>Challenge Introduction (Host: Xiaonan Zhao)</td>
	    <td>15 mins</td>
	</tr>
	<tr>
	    <td>2:15-3:00pm</td>
	    <td>Oral Long Session III (Host: Qipin Chen)</td>
	    <td>45 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">2:15-2:30pm</td>
	    <td style="padding-left: 30px;">Temporally Streaming Audio-Visual Synchronization for Real-World Videos (in person) Presenter: Jordan Voas</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">2:30-2:45pm</td>
	    <td style="padding-left: 30px;">MambaTron: Efficient Cross-Modal Point Cloud Enhancement using Aggregate Selective State Space Modeling (in person) Presenter: Sai Tarun Inaganti</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">2:45-3:00pm</td>
	    <td style="padding-left: 30px;">Diffusion Prism: Enhancing Diversity and Morphology Consistency in Mask-to-Image Diffusion (in person) Presenter: Hao Wang</td>
	    <td style="padding-left: 30px;">15 mins</td>
	</tr>
	<tr>
	    <td>3:00-3:45pm</td>
	    <td>Coffee Break</td>
	    <td>45 mins</td>
	</tr>
	<tr>
	    <td>3:45-4:20pm</td>
	    <td>Oral Short Session II (Host: Joe Liu)</td>
	    <td>35 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">3:45-3:50pm</td>
	    <td style="padding-left: 30px;">Mahalanobis k-NN: A Statistical Lens for Robust Point-Cloud Registrations (in person) Presenter: Tejas Anvekar</td>
	    <td style="padding-left: 30px;">5 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">3:50-3:55pm</td>
	    <td style="padding-left: 30px;">Improving Human Pose-Conditioned Generation: Fine-tuning ControlNet Models with Reinforcement Learning (online) Presenter: Jeonghwan Lee</td>
	    <td style="padding-left: 30px;">5 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">3:55-4:00pm</td>
	    <td style="padding-left: 30px;">PQD: POST-TRAINING QUANTIZATION FOR EFFICIENT DIFFUSION MODELS (online) Presenter: Jiaojiao Ye</td>
	    <td style="padding-left: 30px;">5 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">4:00-4:05pm</td>
	    <td style="padding-left: 30px;">High-Fidelity 4x Neural Reconstruction of Real-time Path Traced Images (in person) Presenter: Zhiqiang Lao</td>
	    <td style="padding-left: 30px;">5 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">4:05-4:10pm</td>
	    <td style="padding-left: 30px;">Confident Pseudo-labeled Diffusion Augmentation for Canine Cardiomegaly Detection (online) Presenter: Youshan Zhang</td>
	    <td style="padding-left: 30px;">5 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">4:10-4:15pm</td>
	    <td style="padding-left: 30px;">Revealing Palimpsests with Latent Diffusion Models: A Generative Approach to Image Inpainting and Handwriting Reconstruction (in person) Presenter: Mahdi Champour</td>
	    <td style="padding-left: 30px;">5 mins</td>
	</tr>
	<tr>
	    <td style="padding-left: 30px;">4:15-4:20pm</td>
	    <td style="padding-left: 30px;">LS-GAN: Human Motion Synthesis with Latent-space GANs (in person) Presenter: Avinash Amballa</td>
	    <td style="padding-left: 30px;">5 mins</td>
	</tr>
	<tr>
	    <td>4:20-4:30pm</td>
	    <td>Closing Remarks (Host: Joe Liu)</td>
	    <td>10 mins</td>
	</tr>
	<tr>
	    <td>4:30-5:30pm</td>
	    <td>Poster Session</td>
	    <td>60 mins</td>
	</tr>
    </table>

    <table>
        <tr>
            <td>
        Zoom Information for virtual presentations:
            </td>
        </tr>
        <tr>
            <td>
                
            <p>Topic: VAQ Workshop - WACV2025</p>
            <p>Time: Feb 28, 2025 08:15 AM </p>
            
            <p>Join Zoom Meeting</p>
	    <p>https://us06web.zoom.us/j/82700199161?pwd=SOAhhH2uWedGxcxvjn2vqRQD8tJmDW.1</p>
            
            <!--<p> ID: 833 6365 3781</p>-->
            <!--<p>Passcode: 500582</p>-->
            
            <!--<p>---</p>-->
            
            <!--<p> tap mobile</p>-->
            <!--<p>+12532158782,,83363653781#,,,,*500582# US (Tacoma)</p>-->
            <!--<p>+12532050468,,83363653781#,,,,*500582# US</p>-->
            
            <!--<p>---</p>-->
            
            <!--<p>Dial by your location</p>-->
            <!--<p>• +1 253 215 8782 US (Tacoma)</p>-->
            <!--<p>• +1 253 205 0468 US</p>-->
            <!--<p>• +1 719 359 4580 US</p>-->
            <!--<p>• +1 346 248 7799 US (Houston)</p>-->
            <!--<p>• +1 669 444 9171 US</p>-->
            <!--<p>• +1 669 900 6833 US (San Jose)</p>-->
            <!--<p>• +1 929 436 2866 US (New York)</p>-->
            <!--<p>• +1 301 715 8592 US (Washington DC)</p>-->
            <!--<p>• +1 305 224 1968 US</p>-->
            <!--<p>• +1 309 205 3325 US</p>-->
            <!--<p>• +1 312 626 6799 US (Chicago)</p>-->
            <!--<p>• +1 360 209 5623 US</p>-->
            <!--<p>• +1 386 347 5053 US</p>-->
            <!--<p>• +1 507 473 4847 US</p>-->
            <!--<p>• +1 564 217 2000 US</p>-->
            <!--<p>• +1 646 931 3860 US</p>-->
            <!--<p>• +1 689 278 1000 US</p>-->
            
            <!--<p>Meeting ID: 833 6365 3781</p>-->
            <!--<p>Passcode: 500582</p>-->
            
            <!--<p>Find your local number: https://us06web.zoom.us/u/kd6Xbti4fC</p>-->
        </td>
            
        </tr>
    </table>
</section>

<section id="Keynotes">
    <h2>Keynotes</h2>

    <div>
	<h3>Keynote Speaker: Kevin Bowyer </h3>
        
        <!-- Speaker Photo -->
	<img class="circular--square" src="keynote_photo.png" alt="Keynote Speaker">

        <!-- Talk Title -->
	<h3>Title: "What makes a good quality face recognition training set?"</h3>

        <!-- Abstract -->
	<p>
	    <strong>Abstract:</strong> This talk will start with comments on web-scraped, in-the-wild training sets.  And possibly also on CVPR and FG reviewing.  Then we will touch on the “identity problem” in training sets of images of persons who don’t exist.  Lastly, we will describe an example of a training set with targeted synthetic enhancements as a means of training set augmentation to solve a specific problem.  This talk should be entertaining and informative for a broad audience.
	</p>

        <!-- Bio -->
	<p>
	    <strong>Bio:</strong> Kevin Bowyer is the Schubmehl-Prein Family Professor of Computer Science and Engineering at the University of Notre Dame.  He is a Fellow of the AAAS, IEEE and IAPR, past EIC of the IEEE Transactions on Pattern Analysis and Machine Intelligence and the IEEE Transactions on Biometrics, Behavior, and Identity Science, recipient of a Technical Achievement Award from the IEEE Computer Society, and of the Meritorious Service Award and the Leadership Award from the IEEE Biometrics Council.
	</p>
    </div>

    <!--<hr>-->

    <!--<div>-->
        <!--<h3>Keynote Speaker: Andrew Segall</h3>-->
        
        <!--[> Speaker Photo <]-->
        <!--[> <img  class="circular--square" src="speaker_photo_3.jpg" alt="Keynote Speaker"> <]-->

        <!--[> Talk Title <]-->
        <!--<h3>Title: "Recent Advances in Deep Learning for Video Compression"</h3>-->

        <!--[> Abstract <]-->
        <!--<p>-->
            <!--<strong>Abstract:</strong> The field of image and video quality is rapidly advancing due to improvements in computer vision, machine learning and neural network design.  However, in many cases, these quality methods are used to evaluate data that has been compressed. -->
             <!--And the compression of these images and video is also advancing due to the same improvements in computer vision, machine learning, and neural networks.  In this talk, we survey recent developments in image and video coding with an emphasis on the use of deep learning.  -->
             <!--Both end-to-end solutions as well as enhancements to existing systems are included.  Additionally, recent efforts to create data sets sampling these methods are introduced.-->
        <!--</p>-->

        <!--[> Bio <]-->
        <!--<p>-->
            <!--<strong>Bio:</strong> Andrew Segall is currently the Head of Video Coding Standards at Amazon Prime Video. Previously, he was a Director at Sharp Labs of America, where he led the Department of Systems, Algorithms and Services while simultaneously holding the position of Distinguished Scientist at Sharp Corporation. -->
            <!--He is an active participant in the international standardization community and has developed and contributed technology to the Versatile Video Coding (VVC), High Efficiency Video Coding (HEVC), Advanced Video Coding (H.264/AVC), and ATSC 3.0 projects. -->
            <!--He currently serves as co-chair of the Neural Network Video Coding activity in the Joint Video Experts Team (JVET) of ITU-T SG16 Question 6 and ISO/IEC JTC1/SC29/WG5, HDR Chair for the MPEG Visual Quality Assessment Advisory Group (ISO/IEC JTC1/SC29/AG5), and represents Amazon on the Alliance for Open Media (AOM) Steering Committee.  -->
            <!--He received his B.S. and M.S. degrees in electrical engineering from Oklahoma State University, and his Ph.D. degree in electrical engineering from Northwestern University.-->
        <!--</p>-->
    <!--</div>-->

</section>

    <section id="Submission">
        <h2>Submission Guidelines and Review Process:</h2>
        <ul>
            <li>Authors are encouraged to submit high-quality, original (i.e., not been previously published or accepted for publication in substantially similar form in any peer-reviewed venue including journal, conference or workshop) research.
            <li>All submissions should follow the same template as for the main WACV2025 conference. Please refer to the WACV Author Kit (<a href="https://www.overleaf.com/latex/templates/wacv-2025-author-kit-template/zfydvwqrjmsb">Overleaf template</a>, <a href="https://www.dropbox.com/scl/fi/su44zgdhrzik26p2xu37k/WACV-2025-Author-Kit-Template.zip?rlkey=5qcfimjhxnmx3wlyk7yhk8wg7&e=1&dl=0">ZIP Archieve</a>). 
            <li>The main paper has an 8-page limit, references do not count toward this. There is no limit on the number of pages in the supplementary material. Only pdf files are accepted.
            <li>Unlike the main conference(WACV2025), the review process for this workshop has only one round, and is single-blind. Authors do not have to be anonymized when submitting their work.
	    <!--<li>Please submit your paper via this link: <a href="https://cmt3.research.microsoft.com/WVAQ2024">https://cmt3.research.microsoft.com/WVAQ2024</a>-->
	    <li>Please submit your paper via this link: <a href="https://cmt3.research.microsoft.com/ImageQuality2025">https://cmt3.research.microsoft.com/ImageQuality2025</a>
            <li>Authors of accepted papers will be notified via email by: <b> 5 January, 2025 </b>
        </ul>
    </section>

    <!--<section id="Competition">-->
    <!--<h2>Competition</h2>-->
	<!--<h3> Overview </h3>-->
		<!--<p> -->
		<!--We are excited to announce the Cross Domain Logo Recognition Grand Challenge, to be hosted as part of the WACV 2025 workshop. This challenge aims to push the boundaries of computer vision and machine learning in tackling one of the most crucial yet challenging tasks in visual brand identity: logo recognition across diverse domains.-->

		<!--Logo recognition plays a pivotal role in numerous industries, including brand management, marketing analytics, intellectual property protection, and e-commerce. The ability to accurately identify logos in various contexts is essential for monitoring brand presence, tracking marketing effectiveness, detecting counterfeit products, and enhancing user experiences in visual search applications. Despite its significance, logo recognition remains a formidable challenge in both academia and industry, particularly when dealing with large-scale, real-world scenarios. Logo detection is challenging due to the vast diversity in appearance, scale, perspective, and potential distortions or occlusions. These factors make it difficult for systems to consistently recognize logos across various products, environments, and viewing conditions.-->

		<!--To further elevate the challenge, we are introducing a unique aspect that mirrors real-world scenarios: <b> recognizing new logos based solely on their design templates </b>. While humans can easily bridge this domain gap and identify logos on actual products after seeing only the original design, machine learning models struggle to generalize across such disparate domains. This aspect of the challenge simulates the practical need for systems that can quickly adapt to new brand identities without extensive real-world training data.-->
		<!--</p>-->
	<!--<h3> Challenge Introduction </h3>-->
		<!--<p> -->
		<!--To support this grand challenge, we will be releasing a comprehensive dataset that encompasses a wide range of logos across various product categories and real-world contexts. This dataset will serve as a benchmark for evaluating the performance of participating models in cross-domain logo recognition.-->

		<!--Amazon (as one of the WACV sponsors) will provide cash prizes for the top 3 submissions. Cash prize:-->
		<!--<ul>-->
			<!--<li> First place: TBD-->
			<!--<li> Second place: TBD-->
			<!--<li> Third place: TBD-->
		<!--</ul>-->

		<!--The prize awards will be granted based on performance over an unreleased test set.-->

		<!--The selected winners will have an opportunity to present their work at the <a href="https://wacv2025-image-quality-workshop2.github.io/">4th Workshop on Image/Video/Audio Quality in Computer Vision and Generative AI</a>-->
		<!--</p>-->
	<!--<h3> Dataset Details </h3>-->
		<!--<p> -->
		<!--The <b> Cross-Domain one-shot Logo Recognition </b> Dataset released for this challenge was created by Amazon and is released under the license <b> CC BY SA 4.0 </b>. For more information, please visit <a href="https://www.codabench.org/competitions/4717">codabench</a>. -->
		<!--</p>-->


    <!--<p>-->
        <!--Alongside the workshop we are hosting a Grand Challenge on the identification of audiovisual synchronization (AV-Sync) errors.  -->
        <!--An AV-Sync error, the temporal misalignment of audio relative to the video, is a defect that can have a large impact on a viewer’s Quality of Experience. -->
        <!--The Telecommunications Union’s Rec. ITU-T J.248: Requirements for operational monitoring of video-to-audio delay in the distribution of television programs (2008) -->
        <!--used subjective experiments to determine participants could detect an AV-Sync error if the audio leads the video by greater than 45 ms and if the audio lags the video by greater than 125 ms.-->
    <!--</p>-->
    <!--<p>-->
        <!--The Grand Challenge will consist of two tasks:-->
        <!--<ul>-->
            <!--<li><b> AV-Sync Error Detection </b> - participants will make a binary prediction whether the audio is out-of-sync with the video. -->
                <!--For more information please see: <a href="https://www.codabench.org/competitions/1541/"> https://www.codabench.org/competitions/1541/ </a></li>-->
            <!--<li><b> AV-Sync Error Measurement </b> - participants will predict a quantitative measure of the magnitude of the AV-Sync Error. -->
                <!--For more information please see:  <a href="https://www.codabench.org/competitions/1543/"> https://www.codabench.org/competitions/1543/ </a></li>-->
        <!--</ul>-->
    <!--</p>-->
    <!--<p>-->
        <!--The two tasks will have a combined prize fund of <b style='color:red;'>$12000</b>. -->
        <!--We invite participants to enter one or both of the tasks at the links above with the following timelines:-->
        <!--<ul>-->
            <!--<li><b>2023-10-11</b> - Release of the Training Dataset</li>-->
            <!--<li><b>2023-10-11</b> - Initial Submission Opens</li>-->
            <!--<li><b>2023-12-02 00:00:00 UTC</b> - Challenge Closes (Final Submissions: 1. to leaderboard and 2. summary report, model and weights)</li>-->
            <!--<li><b>2023-12-15</b> - Announcement of Awardees</li>-->
            <!--<li><b>2023-12-22</b> - Workshop Presentation Signup Deadline</li>-->
            <!--<li><b>2024-01-07</b> - WACV 2024 Workshop Day</li>-->
        <!--</ul>-->
    <!--</p>-->
    <!--</section>-->

    <section id="Organizer">
        <h2>Organizers</h2>
        <div class="organizer-row">
            <div class="organizer">
		<img src="organizer_yarong.png" alt="Organizer 1">
                <h3>Yarong Feng</h3>
                <p>Amazon</p>
            </div>
            <div class="organizer">
		<img src="organizer_qipin.png" alt="Organizer 2">
                <h3>Qipin Chen</h3>
                <p>Amazon</p>
            </div>
        </div>
        <div class="organizer-row">
            <div class="organizer">
		<img src="organizer_joe.png" alt="Organizer 3">
                <h3>Joe Liu</h3>
                <p>Amazon</p>
            </div>
            <div class="organizer">
		<img src="organizer_chenge.png" alt="Organizer 4">
                <h3>Chenge Li</h3>
                <p>Amazon</p>
            </div>
            <div class="organizer">
		<img src="organizer_xiaonan.png" alt="Organizer 5">
                <h3>Xiaonan Zhao</h3>
                <p>Amazon</p>
            </div>
        </div>
    </section>

    <section id="Information">
        <h2>Information</h2>
        <p>TBD</p>
        </table>
    </section>

    <section id="contact">
        <h2>Contact Us</h2>
        <p>If you have any questions or inquiries, please contact us at <a href="mailto:wacv2025-image-quality-workshop2@amazon.com">wacv2025-image-quality-workshop2@amazon.com</a>.</p>
    </section>

    <footer>
        <p>&copy; 4th Workshop on Image/Video/Audio Quality in Computer Vision and Generative AI. All rights reserved.</p>
    </footer>
</body>
</html>
